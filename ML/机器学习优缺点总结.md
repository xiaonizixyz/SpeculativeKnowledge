# 一、正则化算法

## 简介

正则方法是另一种方法（通常是回归方法）的拓展，该方法会基于模型复杂性对其进行惩罚，该方法喜欢相对简单能够更好泛化的模型

## 例子

- 岭回归（Ridge Regression）
- 最小绝对收缩与选择算子（LASSO）
- `GLOSSO`
- 弹性网络（Elastic Net）
- 最小角回归（Least-Angle Regression）

## 优点

- 其惩罚会减少过拟合
- 总会有解决方法

## 缺点

- 惩罚会造成欠拟合
- 很难校准

# 二、集成算法

## 简介

- 集成方法是由多个较弱的模型集成模型组，其中的模型可以单独进行训练，并且它们的预测能以某种方式结合起来做出整体预测
- 该算法的主要问题是找出哪些较弱的模型可以结合起来，以及结合的方法。该方法是一个很强大的技术集合，因此很受欢迎

## 例子

- Boosting
- Bootstrapped Aggregation（Bagging）
- AdaBoost
- 层叠泛化（Stacked Generalization）（blending）
- 梯度推进机（Gradient Boosting Machines，GBM）
- 梯度提升回归树（Gradient Boosted Regression Trees，GBRT）
- 随机森林（Random Forest）

## 优点

- 当前最先进的预测几乎都使用了算法集成。该方法比使用单个预测模型预测出来的结果要精确很多

## 缺点

- 需要大量的维护工作

# 三、决策树

## 简介

- 决策树学习使用一个决策树作为一个预测模型，它将对一个`item`(表征在分支上)观察所得映射成关于该`item`的目标值的结论（表征在叶子中）
- 树模型中的目标是可变的，可以采一组有限值，被称为分类树；在这些树结构中，叶子表示类标签，分支表示表征这些类标签的连接的特征

## 例子

- `分类和回归树（Classification and Regression Tree，CART）`
- `Iterative Dichotomiser 3（ID3）`
- `C4.5 和 C5.0（一种强大方法的两个不同版本）`

## 优点

- 容易解释
- 非参数型

## 缺点

- 趋向过拟合
- 可能或陷于局部最小值中
- 没有在线学习

# 四、回归

## 简介

回归是用于估计两种变量之间关系的统计过程。当用于分析因变量和一个、多个自变量之间的关系时，该算法能提供很多建模和分析多个变量的技巧。最常见的，回归分析可以在给定自变量的条件下估计出因变量的条件期望

## 例子

- 普通最小二乘回归（`Ordinary Least Squares Regression，OLSR`）
- 线性回归（`Linear Regression`）
- 逻辑回归（`Logistic Regression`）
- 逐步回归（`Stepwise Regression`）
- 多元自适应回归样条（`Multivariate Adaptive Regression Splines，MARS`）
- 本地散点平滑估计（`Locally Estimated Scatterplot Smoothing，LOESS`）

## 优点

- 直接、快速

## 缺点

- 要求严格的假设
- 需要处理异常值

# 五、人工神经网络

## 简介

人工神经网络是一种模式匹配，常被用于回归和分类问题，它拥有庞大的子域，由数百种算法和各类问题的变体组成

## 例子

- 感知机
- 反向传播
- `Hopfield`网络
- 径向基函数网络（`RBFN`）

## 优点

- 在语音、语义、视觉、各类游戏（如围棋）的任务中表现极好
- 算法可以快速调整，适应新的问题

## 缺点

- 需要大量数据进行训练
- 训练要求很高的硬件配置
- 模型处于黑盒状态，难以理解内部机制
- 元参数与网络拓扑选择困难



# 六、深度学习

# 七、SVM

# 八、降维算法

# 九、聚类算法

# 十、基于实例的算法

# 十一、贝叶斯算法

# 十二、关联规则学习算法

# 十三、图模型